{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Report\n",
    "#### STATS 102 Introduction to Data Science Session 1, 2024 Fall  Due at Oct 17th 1:15pm  beijing time\n",
    "\n",
    "You can zip everything including the data (or box link) and the codes into a zip file and submit through canvas/assignment/final_project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "\n",
    "- Zhiyuan Shen (zs175@duke.edu)\n",
    "- Yifei Wang (yw645@duke.edu)\n",
    "- Nuo Zheng (nz66@duke.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "We strongly recommend you to read our [official article](http://124.223.218.55:850/articles/stats102-final-project.pdf). Also, [video access](http://124.223.218.55:850/videos/stats102-final-project.mp4) is avaliable for everyone. The figures are stored in the cloud server, make sure you have the network access when read this notebook.\n",
    "\n",
    "This paper aims to use the machine learning to predict the run time of 3-colorability on a graph and its tree-decomposition. Thus we can generate multiple tree-decomposition, and find the fastest one then solve it.\n",
    "\n",
    "This project chooses the 3-colorability as an example because it is a relatively easy NP-complete problem. This paper may first introducing some background information and give a fully view of how the project built.\n",
    "\n",
    "This project conducts 3 parts, including data generating part, machine learning part and final test part. All 3 parts will be fully explained below analyses.\n",
    "\n",
    "In conclusion, the paper will show the findings of optimization through machine learning on the tree-decompositions and try to explain the reasons behind such result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Background\n",
    "\n",
    "From the basic algorithm, the most traditional algorithm is to enumerate the colors of all nodes. After that, graph-coloring problem is determined to be the NP-complete problem. Rather than solve the problem straightforward, academic circle tries to reduce the scale of the problem. Tree-decomposition is an effective method to reduce the scale of origin problem.\n",
    "\n",
    "Then, the focus is shifted to the problems on the tree-decompositions. Improved tree-decomposition methods are found to form a more standard and solvable structures.\n",
    "\n",
    "Next, machine learning was introduced into this problem. Machine learning is used to predict the run time of tree-decomposition structure and help problem-solver to dicide to solve which structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Design and Implementation\n",
    "\n",
    "To fully see our project, you may run the below code segment and make sure you are in Linux Unbuntu environment:\n",
    "```\n",
    "# git clone https://github.com/jerry3128/Stats102-Final-Project-Linux.git\n",
    "```\n",
    "\n",
    "However, if you just want to see the result, follow the instruction below this part and you can see it.\n",
    "\n",
    "We generate the data by ourselves. We uniformly randomly generate the graph, and use the combined algorithm to generate multiple tree-decompositions. In total, we generate 2000 graphs and each graph 100 tree-decompositions for training, 200 graphs and each graph 100 tree-decomposition for testing. After that, we try several models and analysis them, detailed information could be found in our articles \"Machine Learning\" part. At last, we compare the Machine Learning predictor and theoretic predictor and random predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.system('wget 124.223.218.55:850/csv/data.tar.gz') # get the full training data.\n",
    "# os.system('wget 124.223.218.55:850/csv/test.tar.gz') # get the full testing data.\n",
    "# os.system('wget 124.223.218.55:850/csv/data.csv') # get the traiining data's csv.\n",
    "# os.system('wget 124.223.218.55:850/csv/test.csv') # get the testing data's csv.\n",
    "\n",
    "# if you are not in Linux environment, you can straightly goes to the link to download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will introduce the features we select and how we implement the machine learning.\n",
    "\n",
    "Apparently, compared to the other features, the \\textit{Tree Width} has much more impact on the run time. Thus we straightly group the data by tree width, and then for each group we do the polynomial linear regression.\n",
    "\n",
    "Here is the features we may use:\n",
    "1. Average Bag Size: Average of size of bags on tree decomposition.\n",
    "2. Decomposition Overhead Ratio: Sum of bag size divided by size of graph.\n",
    "3. Average Depth: Average depth of tree.\n",
    "4. Node percentage: Introduce/Forget/Join/Leaf's Percentage of tree (sum of them is 100\\%).\n",
    "5. Sum of Join node distance: Find all pairs of Join nodes and plus their distances.\n",
    "6. Branching Factor: Average number of childs (exclude leaf nodes).\n",
    "7. Bag Adjacency Factor: For all pair of nodes in a bag, that the adjacencent pairs divided by the total pairs.\n",
    "8. Bag Connectedness Factor: For all pair of nodes in a bag, the reachable pairs divided by the total pairs.\n",
    "9. Bag Neighborhood Coverage Factor: For all bags $B_{x}$, average of nodes that belong to a bag $B_{x}$ or are adjacencent to a node belongs to $B_{x}$\n",
    "\n",
    "After training, we use cross validation to validate each model and select the polynomial linear regression.\n",
    "\n",
    "We select the best models to participate in the final test.\n",
    "\n",
    "Model: For each tree width, we use Linear-Regression to predict the runtime.\n",
    "\n",
    "![separate tree-width, straight polynomial linear regression for run time](http://124.223.218.55:850/articles/model_polyLR.png)\n",
    "\n",
    "Figure 1. separate tree-width, straight polynomial linear regression for run time\n",
    "\n",
    "Also we compare this model with the other models by the square of the difference between the predictions and real value, and we call this.\n",
    "\n",
    "The models for the ln of run time all has sum of square of difference over $10^{6}$, while straightly linear regression the run time only over $10^{5}$.\n",
    "\n",
    "Then, we see that, if we combine all data, the model shows obvious inaccurate in the area that tree width is little.\n",
    "\n",
    "![combine tree-width, straight polynomial linear regression for run time](http://124.223.218.55:850/articles/no_separate_treewidth.png)\n",
    "\n",
    "Figure 2. combine tree-width, straight polynomial linear regression for run time\n",
    "\n",
    "Thus we may choose the separate tree width model as the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusions\n",
    "\n",
    "In this section, we compare efficiency of the\n",
    "- *predictor 1*: machine learning predictor.\n",
    "- *predictor 2*: theory time complexity predictor.\n",
    "- *predictor 3*: average running time.\n",
    "\n",
    "to solve new generated graphs.\n",
    "\n",
    "We will analysis the result and provide the further project focus.\n",
    "\n",
    "![result-1.png](http://124.223.218.55:850/articles/result-1.png)\n",
    "\n",
    "Figure 3. compare the run time\n",
    "\n",
    "The *predictor 1* shows a very high performance averagely. That only cost 87.4\\% run time of *predictor 2* and 91.0\\% run time of *predictor 3*.\n",
    "\n",
    "However, here is 2\\% for this predictor that get huge differences, that the predicted run time is 2 times larger than real run time. Also, we found that the optimization is not so well when the tree width is large, and the efficiency smaller than related works.\n",
    "\n",
    "We thought that, still the predictor has not sufficient data to do learning, especially in the large tree width area. Also, we found that our tree decomposition algorithm is a kind of combined algorithm, so that it is not uniformly random and will have a higher probability to already optimized tree decomposition.\n",
    "\n",
    "Thus we may improve our project by constantly adding data and improve the basic algorithm parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Individual Contribution\n",
    "\n",
    "Write the individual contribution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Abseher, M., Musliu, N., & Woltran, S. (2017). Improving the efficiency of dynamic programming on tree decompositions via machine learning. \n",
    "Journal of Artificial Intelligence Research, 58, 829–858. https://doi.org/10.1613/jair.5312\n",
    "\n",
    "Bodlaender, H. L. (1996). A linear-time algorithm for finding tree-decompositions of small treewidth. SIAM Journal on Computing, 25(6), 1305–1317. https://doi.org/10.1137/s0097539793251219\n",
    "\n",
    "Bodlaender, H. L., & Kloks, T. (1991). Better algorithms for the pathwidth and treewidth of graphs. Lecture Notes in Computer Science, 544–555. https://doi.org/10.1007/3-540-54233-7_162\n",
    "\n",
    "Gutin, G., & Szeider, S. (2013). Parameterized and exact computation: 8th International Symposium, IPEC 2013, Sophia Antipolis, France, September 4-6, 2013, revised selected papers. Springer.\n",
    "\n",
    "Kangas, K., Koivisto, M., & Salonen, S. (2019). A faster tree-decomposition based algorithm for counting linear extensions. Algorithmica, 82(8), 2156–2173. https://doi.org/10.1007/s00453-019-00633-1\n",
    "\n",
    "Kloks, T. (1994). Treewidth computations and approximations ton KLOKS. Springer.\n",
    "\n",
    "Matoušek, J., & Thomas, R. (1991). Algorithms finding tree-decompositions of graphs. Journal of Algorithms, 12(1), 1–22. https://doi.org/10.1016/0196-6774(91)90020-y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Code and extra figures can be put here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
